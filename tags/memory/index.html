<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tag: memory | Yassin java development blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Yassin java development blog">
<meta property="og:url" content="http://yassinm.github.io/tags/memory/">
<meta property="og:site_name" content="Yassin java development blog">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yassin java development blog">
<meta name="twitter:description">

  
    <link rel="alternative" href="/atom.xml" title="Yassin java development blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">

  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Yassin java development blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><input type="submit" value="&#xF002;" class="search-form-submit"><input type="hidden" name="q" value="site:http://yassinm.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-walking-down-memory-lane-03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/10/06/walking-down-memory-lane-03/" class="article-date">
  <time datetime="2014-10-06T04:00:00.000Z" itemprop="datePublished">Oct 6 2014</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/10/06/walking-down-memory-lane-03/">Walking down memory lane (part III)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>If you remember our previous <a href="/2014/09/25/walking-down-memory-lane-02">article</a> we used a central memory location to share data between two separate threads each running either in the same process or across separate processes. These processes could even be located on separate machine or even be written in two different languages.</p>
<img src="/images/walking-down-memory-lane-03-000.png" width="550">

<p>From the above you can see that we only have one thread writing to the shared data. The other thread is only reading this shared data once it is notified that the data was correctly published by the writer thread. Now the question is how do these threads synchronize access to this shared memory location with minimal contention ?</p>
<h2 id="Lock_free_rings_to_the_rescue">Lock free rings to the rescue</h2>
<p>If we need to access this data concurrently we need to let the two separate threads agree (more like synchronize) access based on a locking mechanism. The trick here is to piggy back on the memory model provided by the processor. The cache coherency on x86 systems provides a happens-before guaranty when the data in a certain memory location is modified. However, if the memory is written using the <a href="http://mechanical-sympathy.blogspot.ca/2011/09/single-writer-principle.html" target="_blank" rel="external">Single Writer Principle</a> that data is not propagated forcibly each time a write occurs to invalidate other caches. This basically means that, If the publishing thread writes to this memory location the other thread(s)/core(s) will see this change <em>at some time in the future</em> (and anything that happened before it) without impeding or affecting the publishing thread. As such, the publishing thread can keep updating that location. For the gory details i will point you to this article on <a href="http://mechanical-sympathy.blogspot.ca/2013/02/cpu-cache-flushing-fallacy.html" target="_blank" rel="external">CPU Cache</a> and this one on <a href="http://mechanical-sympathy.blogspot.ca/2011/07/memory-barriersfences.html" target="_blank" rel="external">memory barriers and fences</a>.</p>
<img src="/images/walking-down-memory-lane-03-001.png" width="550">

<p>As can be seen above the two threads behaviour looks, conceptually, like a reservoir with one thread filling it from the top while the other thread is draining it. The memory is treated as a single contiguous stream of data with each message having a unique incrementally sequential location. The reader thread knows that the messages coming down the pipe have the same size are all numbered in a sequential manner and keeps track of the last message position that was fetched from the pipe (head). On the other hand, the publishing thread also knows the sequence number of the last message written to the pipe (tail). Both sequences are augmented incrementally and independently via lock free based mechanism. The algorithm for inserting new elements in this queue must enforce a limit since we only have a very finite space for storing things. Ergo the notion of a circular ring buffer. The writer thread needs to make sure no new elements are inserted when the circular ring buffer is full. On a similar note, the reader thread also makes sure that if there are no new elements added since the last fetch it can spin lock waiting for new elements to be inserted. This algorithm is said to be lock free but not necessarily wait free. Meaning that we can bail out and go do other things when the queue is full/empty , making thus the algorithm as wait free, or if there is nothing else to do we can do a spin lock until the situation is resolved. Meaning we lose the wait free portion but we are still lock free.</p>
<p>One can easily imagine how this buffer can be seen by multiple reader threads with one single publisher making broadcast or parallel type messaging. It is only a matter of adding more head/tail addresses and managing that accordingly when we are adding/removing items in the queue. Once the messages are read by multiple consumer threads they can all act on the same data in a parallel fashion or each consumer thread can be responsible for only part of the sequences based on a partitioning scheme. One particular partitioning technique championed by the <a href="http://lmax-exchange.github.io/disruptor/" target="_blank" rel="external">Disruptor</a> is to use a modulo for each reader so that for example odd sequence messages are handled by one reader while the others are handled by another reader. But we still get the same benefits in that the actual data will be paged by the kernel and available right away without incurring unnecessary cache misses. Once a block of data is written to memory we could use something like <a href="https://www.ibm.com/developerworks/linux/library/j-zerocopy/" target="_blank" rel="external">transferTo</a> to finally achieve a zero copy like network transfer in Java while the publishing thread continues to write new data on a new location. the fact remains that each particular reader thread is effectively free to do what it pleases with the block of data without waiting or synchronizing with other threads.</p>
<p>Furthermore, as we said before, the consumer thread can be anything that can access this memory locations sequentially. This scheme can be used to not only communicate with lower level c/c++ code (or any code written in any language for that matter) but it can also be used to send data directly to GPUs running on the same machine. Once a sufficient amount of data is written by the thread publishing this block of data it can be copied over using DMA over the graphic processing card for example. The same obviously goes for sending/receiving data over high speed NIC cards or proprietary high speed transports like <a href="http://solacesystems.com/" target="_blank" rel="external">solace</a> or <a href="http://www.solarflare.com/" target="_blank" rel="external">solarflare</a>.</p>
<h2 id="Lets_share_some_data_!">Lets share some data !</h2>
<p>I uploaded the code for this article on <a href="https://github.com/yassinm/tools.mlane" target="_blank" rel="external">github</a>. The application is using shared memory mapped files for the memory location. When i run the publisher and subscriber for a 50Million 8 byte (long) one way message stream (from publisher to subscriber only) on a AWS m3.2xlarge instance i am consistently getting ~140 Million messages per second. The test is repeated for 30times and averaged. On my AMD 4 core laptop i am getting a mere 13M messages per second. This is before we start talking about thread affinity and cpu isolation. If you can run this code on your own hardware give me a shout! </p>
<!-- 

[Part I](/2014/09/21/walking-down-memory-lane-01)
[Part II](/2014/09/25/walking-down-memory-lane-02)
[Part III](/2014/10/06/walking-down-memory-lane-03)


-->
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yassinm.github.io/2014/10/06/walking-down-memory-lane-03/" data-id="akgmdopj4tuc8won" class="article-share-link">Share</a>
      
        <a href="http://yassinm.github.io/2014/10/06/walking-down-memory-lane-03/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/memory/">memory</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-walking-down-memory-lane-02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/09/25/walking-down-memory-lane-02/" class="article-date">
  <time datetime="2014-09-25T04:00:00.000Z" itemprop="datePublished">Sep 25 2014</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/09/25/walking-down-memory-lane-02/">Walking down memory lane (part II)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>As per our previous <a href="/2014/09/21/walking-down-memory-lane-01">article</a> we identified that we need a way to serialize objects into memory in a very efficient manner. To recap here is a picture showing the steps we need to remove when sending/receiving objects over the network.</p>
<img src="/images/walking-down-memory-lane-02-000.png" width="300">

<p>FTR we are still not interested in solving how these troublesome bytes are transferred over the network from one process to another once the data is ready to be transferred. For this article we are leaving this aspect aside and will only concentrate on efficiently storing those objects in memory and manipulating them correctly.</p>
<h2 id="Unsafe_relationships_with_data">Unsafe relationships with data</h2>
<p>Like we said earlier java has a dark side if you want to really start fiddling with bytes in an unsafe and unprotected manner. This is called (quite rightly) sun.misc.Unsafe. For the gory details i will refer you to this <a href="http://mechanical-sympathy.blogspot.ca/2012/07/native-cc-like-performance-for-java.html" target="_blank" rel="external">article</a>. What we gain immediately by using this unsafe method are :</p>
<ol>
<li>We finally have a way of accessing bytes for read and writes purposes directly just like we used to in c/c++. We can also do so very efficiently and in a very high speed manner.</li>
<li>The code using these objects is not aware of what is happening behind the scene.</li>
<li>For the sake of cache coherency, it is much better to access the memory in a single stride fashion so that we are not jumping back and forth in memory.</li>
</ol>
<h2 id="Hiding_behind_an_interface">Hiding behind an interface</h2>
<p>What we achieved, instantly, by using the “unsafe” method is a read/write through interface. The block of memory behind the object is effectively  acting as a cache that we use to read/write behind the scene. The fact that we are dealing with unsafe is only an implementation specific issue. All the code using this interface is unaware of what is happening.</p>
<img src="/images/walking-down-memory-lane-02-001.png" width="480">

<p>This effectively means that the memory backing these objects could come from anywhere in the system ! We are not limited by the limitations of heap memory and its cumbersome garbage collection schemes. The gates are open for an infinite amount of unmanaged off-heap memory !  In fact, we are not even limited by the memory available on the process altogether. We could use a shared memory scheme where all the data could be read/written by a separate locally running process. Or for that matter anything that was published by the kernel in a memory location somewhere on the system !!! As long as we can address the memory in a progressive single stride fashion  we can effectively address anything located locally or somewhere else on the universe. More on this later … </p>
<h2 id="Sharing_is_messaging_!">Sharing is messaging !</h2>
<p>As we said above the memory that is living behind the proxy object could also be a shared memory location. As long as we take care of how we are accessing these memory locations concurrently we can effectively send messages between local threads running within the same process !</p>
<img src="/images/walking-down-memory-lane-02-002.png" width="550">

<p>In the same vein it is not a big stretch to see how this technique could be applied to threads running across a multitude of processes running within the same machine or running on different geographically located data centres as part of a big cluster. It is only a matter of making sure the objects see the exact same data copied verbatim across those process boundaries. The biggest technical hurdle that is left to overcome is the synchronization mechanism we will use to make sure we are accessing these objects safely. Of course it will also have to be a very fast synchronization mechanism or we will lose the benefit of doing all these magical memory fiddling contraptions!</p>
<p>Furthermore, if we keep accessing memory with the <a href="http://mechanical-sympathy.blogspot.ca/2011/09/single-writer-principle.html" target="_blank" rel="external">Single Writer Principle</a> in mind we might be able to share this memory location with more than one reader in a non contention based fashion. To quote that page ‘On x86/x64 <strong>loads can be re-ordered with older stores</strong> according to the memory model so memory barriers are required when multiple threads mutate the same data across cores. The single writer principle avoids this issue because it never has to deal with writing the latest version of a data item that may have been written by another thread and currently in the store buffer of another core”. But let us not go ahead of ourself here, we are not there yet !</p>
<p>One thing that you (maybe) missed to notice is that i completely ignored talking about how we were going to turn our objects to memory bytes via the serialization step which we set out to fix in the beginning of this article. As you can see that step is unnecessary since the code using our fine grained fly weight proxy object is directly reading and writing to the bytes locations. As such, when the user code calls the set functions on the proxy object we are technically performing the serialization to memory right away. Vice versa, when the user calls the get functions available on the proxy object they are effectively de-serializing the object from memory. All of the above is done efficiently and with minimal locking involved … Et voila !</p>
<h2 id="Do_you_know_the_memory_man_?_the_memory_man_?_who_lives_on_Drury_Lane_?">Do you know the memory man ? the memory man ? who lives on Drury Lane ?</h2>
<p>Now one more little titbit remaining is that the process or thread we are sharing our data with does not have to be written in java at all. It could be written in any language binding that can use our proxy object. It can be in Java,c#,c/c++,erlang, ruby,nodejs, python , php … you name it. It just does not matter at all. Like we said before ,for all intents and purposes , it could be coming from the kernel itself. As long as the memory representation and location is followed we can both communicate over this high speed channel. Imagine a world where you could write your database access in Java and your number crunching in c/c++. And they could be both living within the same process or across from two separate data centers !</p>
<h2 id="Conclusion">Conclusion</h2>
<p>We started with solving a simple serialization/de-serialization problem and we are already talking about fixing the inherent communication problems when people want to mix code written for different platforms and with different languages without sacrificing performance. I would say that this side effect is a very beautiful addition to solving our original problem(s). Assuming it works out … obviously !</p>
<p>Until we get there i can always say that “I have a dream … I have a dream that one day down in a process — with its vicious language base discrimination , with its OS having its lips dripping with the words of interposition and nullification — one day right there in a process, little java boys and c# girls will be able to join hands with little c/c++ boys and ruby girls as sisters and brothers. I have a dream that one day every valley shall be exalted, and every hill and mountain shall be made low. The rough places will be plain and the crooked places will be made straight, and the glory of all languages shall be revealed”</p>
<p>So stay tuned for that dream to come true !</p>
<!-- 
The barrier to communication between processes/threads written in different languages has never been a technical one. It has always been, as far as i can remember, based on a lock in to a particular platform and very rigid architectures that was limiting in its acceptance of foreign languages.

[Part I](/2014/09/21/walking-down-memory-lane-01)
[Part II](/2014/09/25/walking-down-memory-lane-02)
[Part III](/2014/10/06/walking-down-memory-lane-03)


-->
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yassinm.github.io/2014/09/25/walking-down-memory-lane-02/" data-id="wm4i9h4f8vjhm4e5" class="article-share-link">Share</a>
      
        <a href="http://yassinm.github.io/2014/09/25/walking-down-memory-lane-02/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/memory/">memory</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-walking-down-memory-lane-01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/09/21/walking-down-memory-lane-01/" class="article-date">
  <time datetime="2014-09-21T04:00:00.000Z" itemprop="datePublished">Sep 21 2014</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/09/21/walking-down-memory-lane-01/">Walking down memory lane (part I)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>One of the topics that you often have to deal with in java, especially in high performance and low latency distributed code, is how you send a set of objects to a worker thread in the most efficient manner. This thread could be running in the same process or on a completely separate machine located somewhere else in your network. This has always been the case for people coming from a c/c++ environment even before the advent of Java. However, Java has steadily gained so much in performance and JIT improvements over the years that nowadays it is on par , in some areas, with c/c++. This of course does not come free and requires a very careful design and a very good understanding of current IPC mechanisms. Moreover, some of the new techniques involving Off-heap memory allocations and lock free queues can even go further. As always, the devil is in the details. In this post i will try to first look at how you can pass data efficiently between different threads/processes and the different issues we have to solve when dealing with the java language.</p>
<h2 id="Serializing/de-serializing">Serializing/de-serializing</h2>
<p>If you want to pass around a block of memory in c/c++ all you need to do on the receiving side is cast the actual memory location to a particular structure and you immediately get an object you can play with. At your own risk of course! This scenario is similar to having all your belts off while driving a formula one vehicle and going the wrong way on a high speed lane … Other cars coming at you being the data that was sent to you. This is why you can refer to this practice as having “unsafe” relations with your data! On the flip side, this does not mean the code receiving the data has to be running within the same process. It could be running somewhere else in the universe as long as the actual data was transported successfully! You still have to take care of the normal Endian-ness of the platform, which is obviously also needed in Java, but that is all that’s required. Technically speaking, there is no need for serialization/de-serialization required since the actual objects are already in bytes format. Unfortunately, we do not have the luxury of these dangerous “Shoot my own feet features” practices in Java. That is, unless you turn yourself into the dark side by using the famously unsafe using sun.misc.Unsafe ! More on this later…</p>
<p>In java , when you pass objects around between threads within the same process you can get away by only passing the reference to the actual object. Obviously, you will have to take great care when the passed object is accessed concurrently across the multiple threads in your process. You can deal with issues like this by implementing one of the concurrent access mechanisms available in the JVM. The problem however comes when your target thread is running in a separate process. In that scenario you need a way of serializing/de-serializing the objects before and after they hit the wire. This ,dealing with objects back and forth, becomes very cumbersome suddenly. </p>
<p>The above additional complexity stems from the fact that we do not have (yet) a way to deal with arrays of structures in java as we do in c/c++. We basically can’t have “unsafe” relations with our data. All objects , including arrays of all types, are passed by reference. As such these objects must all live in the heap and are all susceptible for garbage collection. This basically means that anything you get from the wire has to be “transformed” into actual objects before they are ready for consumption. Lately, the growing majority experience this aspect in Java when dealing with Restful services. An object is first serialized into JSON , sent over the wire via a transport protocol, and then de-serialized when it arrives at its destination before it is passed to the upper application specific code. The simpler the object the easier it is to perform those steps and the faster the overall code becomes. I will not bore you with the multiple technologies available in that arena but i will point you to this <a href="https://code.google.com/p/thrift-protobuf-compare/wiki/Benchmarking" target="_blank" rel="external">comparison</a>. </p>
<h2 id="Memory_control">Memory control</h2>
<p>Unfortunately, there is no particular way one can instruct the JVM on how objects are stored and how they are placed in memory in contiguous or non contiguous fashion. As far as the general programmer is concerned these pesky details are what make the platform “safe”. As such, these details are considered irrelevant for the majority of use cases. This is exactly why it is not needed for a lot of users. The exception is when your code needs to deal with performance critical access to very large amount of objects and you need to have a good control on how these particular objects are stored in memory efficiently. .</p>
<p>Furthermore, the access pattern across different threads and cores accessing the actual fields in your objects can also be problematic. False sharing explained <a href="http://mechanical-sympathy.blogspot.ca/2011/07/false-sharing.html" target="_blank" rel="external">here</a> and <a href="http://mechanical-sympathy.blogspot.ca/2011/08/false-sharing-java-7.html" target="_blank" rel="external">here</a> is a very good case on why it is important to know your memory layout in a performance critical code. Again this is not for everyone! But you can still see why it is imperative to understand the concepts involved when you need to write performance critical code.</p>
<p>To make things even more complicated there is no “traditional” way in Java on how you access bytes directly as soon as they are off the network in a zero copy fashion. Once you get data in a NIC buffer the application has to go through a context switch before data is available in user land. The same thing happens when the bytes are on their way out. There are commercial libraries/hardware to do these but so far this not part of the things available out of the box and definitely not for “free” !</p>
<h2 id="Conclusion">Conclusion</h2>
<p>You would think that someone has already worked out the details for most of these issues and came up with a clean and simplistic way , ala Corba style, for you to send data across different threads/processes and with high speed. Fortunately there are some outstanding libraries out there we will explore in our next articles, but you still have to understand these complexities yourself and you still need to be in the know. Ergo, the need for this first part covering the basics!</p>
<p>To conclude i will say that we need to figure a way to:</p>
<ul>
<li>Turn objects to bytes (and vice versa) in a very high speed fashion</li>
<li>Make sure we have a good control on how the memory is been used even in java.</li>
</ul>
<p>The above outlines only some of the issues i can think of right now. To me these issues constitute a list of requirements i would want my  high speed serializing/de-serializing code to deal with. Do i hear a future framework in java in the horizon that would solve (most of) these issues … that’s a “definite maybe” so stay tuned!</p>
<!-- 

I was introduced to client/servers programming with the advent of Corba and Microsoft DCOM back in the days ... Nostalgia !!! The same issues we have today existed at that time in that an object coming on and off the wire needed to be marshalled/de-marshalled before it was ready for consumption. One interesting aspect was that the more an object had to deal with nested objects the more it became difficult and cumbersome to use.

Moreover, when the object is being de-serialized you will often want to not wait until all the fields are ready for consumption. You will want a reactive style programming where you act on the object as soon as some specific fields are ready and bail out completely if you encounter errors during this phased process. Imagine the amount of boring and repetitive code you will have to write if you had to do this yourself! 

You will think that there has to be a much easier way to do this ??? Why do i need all these layers of abstractions and code if all i want to do is pass some data around ? 
more 

-->
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yassinm.github.io/2014/09/21/walking-down-memory-lane-01/" data-id="mfap7yfckf8dmi3u" class="article-share-link">Share</a>
      
        <a href="http://yassinm.github.io/2014/09/21/walking-down-memory-lane-01/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/memory/">memory</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2014/10/06/walking-down-memory-lane-03/">Walking down memory lane (part III)</a>
          </li>
        
          <li>
            <a href="/2014/09/25/walking-down-memory-lane-02/">Walking down memory lane (part II)</a>
          </li>
        
          <li>
            <a href="/2014/09/21/walking-down-memory-lane-01/">Walking down memory lane (part I)</a>
          </li>
        
          <li>
            <a href="/2013/07/13/first-post/">Starting to blog ... finally</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memory/">memory</a><span class="tag-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/java/" style="font-size: NaNpx;">java</a><a href="/tags/memory/" style="font-size: NaNpx;">memory</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/10/">October 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">September 2014</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/07/">July 2013</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2014 Ymo<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'yassinm';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">

  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>



<script src="/js/script.js" type="text/javascript"></script>


  </div>
</body>
</html>